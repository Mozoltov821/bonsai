#!/usr/bin/env python3
"""
MiMo Audio æœ€å°æ¨ç†è„šæœ¬

ä» test_end_to_end.py æå–çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
- åŠ è½½ audio tokenizer
- åŠ è½½ä¸»æ¨¡å‹
- æ‰§è¡Œæ–‡æœ¬è½¬è¯­éŸ³æ¨ç†
"""

import os
import json
import time
from typing import Optional

import jax
import jax.numpy as jnp
import numpy as np
from flax import nnx


def load_audio_tokenizer(tokenizer_path: str):
    """åŠ è½½éŸ³é¢‘ tokenizer"""
    print(f"ğŸ“¥ åŠ è½½éŸ³é¢‘ tokenizer: {tokenizer_path}")

    from bonsai.models.mimo_audio.mimo_audio_tokenizer_configuration import MiMoAudioTokenizerConfig
    from bonsai.models.mimo_audio.mimo_audio_tokenizer_params import load_tokenizer_weights_from_safetensors

    # åŠ è½½é…ç½®
    config_path = os.path.join(tokenizer_path, "config.json")
    with open(config_path) as f:
        config_dict = json.load(f)

    config_dict['use_sharding'] = False  # å•å¡æ¨¡å¼
    config = MiMoAudioTokenizerConfig(**config_dict)

    print(f"   - ç¼–ç å™¨å±‚æ•°: {config.encoder_layers}")
    print(f"   - è§£ç å™¨å±‚æ•°: {config.decoder_layers}")
    print(f"   - é‡åŒ–å™¨æ•°é‡: {config.num_quantizers}")
    print(f"   - é‡‡æ ·ç‡: {config.sampling_rate} Hz")

    # åŠ è½½æƒé‡
    safetensors_path = os.path.join(tokenizer_path, "model.safetensors")
    start_time = time.time()

    tokenizer_model = load_tokenizer_weights_from_safetensors(
        config=config,
        safetensors_path=safetensors_path,
        dtype=jnp.float32,  # Tokenizerå¿…é¡»ç”¨float32
        mesh=None,
        rngs=nnx.Rngs(0),
    )

    load_time = time.time() - start_time
    print(f"âœ… Tokenizer åŠ è½½å®Œæˆ ({load_time:.2f}s)\n")

    return tokenizer_model, config


def load_main_model(model_path: str):
    """åŠ è½½ MiMo Audio ä¸»æ¨¡å‹"""
    print(f"ğŸ“¥ åŠ è½½ä¸»æ¨¡å‹: {model_path}")

    from bonsai.models.mimo_audio.mimo_audio_configuration import MiMoAudioConfig, MiMoAudioArguments
    from bonsai.models.mimo_audio.params import create_model_with_weights
    from transformers import AutoTokenizer

    # åŠ è½½é…ç½®
    config_path = os.path.join(model_path, "config.json")
    with open(config_path) as f:
        config_dict = json.load(f)

    print(f"   - æ¨¡å‹ç±»å‹: {config_dict.get('model_type')}")
    print(f"   - éšè—å±‚å¤§å°: {config_dict.get('hidden_size')}")
    print(f"   - å±‚æ•°: {config_dict.get('num_hidden_layers')}")

    # åˆ›å»ºé…ç½®
    config_kwargs = {k: v for k, v in config_dict.items() if k in MiMoAudioConfig.__dataclass_fields__}
    config = MiMoAudioConfig(**config_kwargs)

    # ä»tokenizerè·å–special token IDs
    text_tokenizer = AutoTokenizer.from_pretrained(model_path)

    args = MiMoAudioArguments(
        model_name_or_path=model_path,
        sosp_idx=text_tokenizer.convert_tokens_to_ids("<|sosp|>"),
        eosp_idx=text_tokenizer.convert_tokens_to_ids("<|eosp|>"),
        sostm_idx=text_tokenizer.convert_tokens_to_ids("<|sostm|>"),
        eostm_idx=text_tokenizer.convert_tokens_to_ids("<|eostm|>"),
        eot_idx=text_tokenizer.convert_tokens_to_ids("<|eot|>"),
        empty_idx=text_tokenizer.convert_tokens_to_ids("<|empty|>"),
    )

    print(f"   - SOSTM: {args.sostm_idx}")
    print(f"   - EOSTM: {args.eostm_idx}")
    print(f"   - Empty: {args.empty_idx}")

    # åŠ è½½æ¨¡å‹
    start_time = time.time()
    model = create_model_with_weights(
        model_path=model_path,
        config=config,
        args=args,
        rngs=nnx.Rngs(0),
        mesh=None,
    )
    load_time = time.time() - start_time

    print(f"âœ… ä¸»æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}s)\n")

    return model, config, args, text_tokenizer


def insert_between(tokens: list, group_size: int, fill_value: int) -> list:
    """åœ¨tokensä¹‹é—´æ’å…¥å¡«å……å€¼"""
    if group_size <= 1:
        return tokens

    result = []
    for token in tokens:
        result.append(token)
        result.extend([fill_value] * (group_size - 1))

    return result


def run_inference(
    main_model,
    tokenizer_model,
    text_tokenizer,
    config,
    args,
    tokenizer_config,
    text_to_speak: str,
    max_steps: int = 100,
    output_dir: str = "test_outputs"
):
    """æ‰§è¡Œæ–‡æœ¬è½¬è¯­éŸ³æ¨ç†"""
    print("=" * 70)
    print("ğŸ™ï¸  å¼€å§‹æ¨ç†")
    print("=" * 70)

    from bonsai.models.mimo_audio.modeling import forward_jit, MiMoSampler
    from bonsai.models.mimo_audio.mimo_audio_configuration import MiMoSamplerConfig

    audio_channels = main_model.audio_channels
    group_size = main_model.group_size
    batch_size = 1

    # å‡†å¤‡è¾“å…¥ - ä½¿ç”¨TTSæ ¼å¼
    tts_template = "è¯·å°†è¿™æ®µæ–‡å­—è½¬æ¢ä¸ºè¯­éŸ³"
    chat_text = f"<|im_start|>user\n{tts_template}: {text_to_speak}<|im_end|>\n<|im_start|>assistant\n<|sostm|>"

    print(f"\nğŸ“ è¾“å…¥æ–‡æœ¬: {text_to_speak}")
    print(f"   TTSæ¨¡æ¿: {tts_template}")

    # Tokenize
    text_tokens_raw = text_tokenizer.encode(chat_text)
    text_tokens_with_spacing = insert_between(text_tokens_raw, group_size, -100)

    # è®¡ç®—num_groups
    num_groups = len(text_tokens_with_spacing) // group_size
    if len(text_tokens_with_spacing) % group_size != 0:
        text_tokens_with_spacing.extend([-100] * (group_size - len(text_tokens_with_spacing) % group_size))
        num_groups = len(text_tokens_with_spacing) // group_size

    print(f"   åŸå§‹tokens: {len(text_tokens_raw)}")
    print(f"   é—´éš”åtokens: {len(text_tokens_with_spacing)}")
    print(f"   ç»„æ•°: {num_groups}")

    # åˆ›å»ºè¾“å…¥
    input_shape = (batch_size, audio_channels + 1, num_groups * group_size)
    input_ids = jnp.zeros(input_shape, dtype=jnp.int32)

    # è®¾ç½®æ–‡æœ¬é€šé“
    input_ids = input_ids.at[0, 0, :].set(jnp.array(text_tokens_with_spacing))

    # è®¾ç½®éŸ³é¢‘é€šé“ä¸ºempty_ids
    for ch in range(1, audio_channels + 1):
        channel_empty_id = main_model.speech_empty_ids[ch - 1]
        audio_empty_tokens = jnp.full((num_groups * group_size,), channel_empty_id, dtype=jnp.int32)
        input_ids = input_ids.at[0, ch, :].set(audio_empty_tokens)

    # åˆå§‹åŒ–cache
    cache = main_model.model.init_cache(
        main_model.qwen2_config,
        batch_size,
        num_groups,
        generate_steps=max_steps,
        dtype=jnp.bfloat16,
    )

    # JITé¢„çƒ­
    print("\nâš¡ JITé¢„çƒ­ä¸­...")
    warmup_cache = main_model.model.init_cache(
        main_model.qwen2_config, 1, 1, 0, jnp.bfloat16
    )
    warmup_input = jnp.zeros((1, audio_channels + 1, group_size), dtype=jnp.int32)
    _, _, _ = forward_jit(main_model, warmup_input, warmup_cache, pad_id=0)
    print("âœ… JITé¢„çƒ­å®Œæˆ\n")

    # åˆ›å»ºsamplers
    text_sampler = MiMoSampler(MiMoSamplerConfig(temperature=0.6, top_p=1.0, do_sample=True))
    audio_sampler = MiMoSampler(MiMoSamplerConfig(temperature=0.9, top_p=0.95, do_sample=True))

    # Prefill
    print("ğŸ”„ æ‰§è¡Œprefill...")
    start_time = time.time()

    pad_id = text_tokenizer.pad_token_id
    text_logits, local_hidden_states, cache = forward_jit(
        main_model, input_ids, cache, pad_id
    )

    print(f"âœ… Prefillå®Œæˆ ({time.time() - start_time:.3f}s)")
    print(f"   æ–‡æœ¬logits: {text_logits.shape}")
    print(f"   å±€éƒ¨éšè—çŠ¶æ€: {local_hidden_states.shape}\n")

    # ç”Ÿæˆå¾ªç¯
    print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ (æœ€å¤š{max_steps}æ­¥)...\n")

    generated_text_tokens = []
    generated_audio_tokens_list = []

    rng_key = jax.random.key(42)
    empty_idx = args.empty_idx

    for step in range(max_steps):
        # é‡‡æ ·æ–‡æœ¬token
        key, subkey = jax.random.split(rng_key)
        logits_2d = text_logits[0, 0:1, :]
        next_text_token = text_sampler.sample(logits_2d, subkey)
        next_text_token_int = int(next_text_token[0])
        generated_text_tokens.append(next_text_token_int)

        # æ¯10æ­¥æ‰“å°è¿›åº¦
        if step % 10 == 0:
            token_type = "EOSTM" if next_text_token_int == args.eostm_idx else \
                        "EMPTY" if next_text_token_int == empty_idx else \
                        "TEXT"
            print(f"   æ­¥éª¤ {step + 1}: token={next_text_token_int} ({token_type})")

        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        if next_text_token_int == args.eostm_idx:
            print(f"\nâœ… ç”ŸæˆEOSTMï¼Œåœæ­¢ç”Ÿæˆ (æ­¥éª¤{step + 1})")
            break
        if next_text_token_int == text_tokenizer.eos_token_id:
            print(f"\nâœ… ç”ŸæˆEOSï¼Œåœæ­¢ç”Ÿæˆ (æ­¥éª¤{step + 1})")
            break

        # ç”ŸæˆéŸ³é¢‘æˆ–ä½¿ç”¨empty_ids
        audio_tokens = None

        if next_text_token_int != empty_idx:
            # ä¸ç”ŸæˆéŸ³é¢‘
            for t in range(group_size):
                audio_tokens_step = jnp.array(main_model.speech_empty_ids)
                generated_audio_tokens_list.append(audio_tokens_step)
        else:
            # ç”ŸæˆéŸ³é¢‘
            key, subkey = jax.random.split(key)
            audio_tokens = main_model.local_forward(
                local_hidden_states,
                subkey,
                audio_sampler
            )

            for t in range(group_size):
                audio_tokens_step = audio_tokens[0, t, :]
                generated_audio_tokens_list.append(audio_tokens_step)

        rng_key = key

        # å‡†å¤‡ä¸‹ä¸€æ­¥è¾“å…¥
        next_input = jnp.zeros((batch_size, audio_channels + 1, group_size), dtype=jnp.int32)

        # æ–‡æœ¬é€šé“
        for i in range(group_size):
            next_input = next_input.at[0, 0, i].set(next_text_token[0])

        # éŸ³é¢‘é€šé“
        if audio_tokens is None:
            for ch in range(audio_channels):
                channel_empty_id = main_model.speech_empty_ids[ch]
                for i in range(group_size):
                    next_input = next_input.at[0, ch + 1, i].set(channel_empty_id)
        else:
            for ch in range(audio_channels):
                for i in range(group_size):
                    next_input = next_input.at[0, ch + 1, i].set(audio_tokens[0, i, ch])

        # ç»§ç»­ç”Ÿæˆ
        text_logits, local_hidden_states, cache = forward_jit(
            main_model, next_input, cache, pad_id
        )

    inference_time = time.time() - start_time
    print(f"\nâœ… æ¨ç†å®Œæˆ (æ€»è€—æ—¶: {inference_time:.3f}s)")
    print(f"   ç”Ÿæˆäº† {len(generated_text_tokens)} ä¸ªtokens\n")

    # è§£ç æ–‡æœ¬
    print("=" * 70)
    print("ğŸ“„ ç”Ÿæˆç»“æœ")
    print("=" * 70)

    try:
        generated_text = text_tokenizer.decode(generated_text_tokens, skip_special_tokens=True)
        print(f"\næ–‡æœ¬: {generated_text}")
    except Exception as e:
        print(f"\næ–‡æœ¬è§£ç å¤±è´¥: {e}")
        print(f"Tokens: {generated_text_tokens}")

    # å¤„ç†éŸ³é¢‘
    print("\nğŸ”Š å¤„ç†éŸ³é¢‘...")

    try:
        # è½¬æ¢ä¸ºæ•°ç»„
        audio_tokens_array = jnp.stack(generated_audio_tokens_list, axis=0).T
        print(f"   åŸå§‹tokens: {audio_tokens_array.shape}")

        # è¿‡æ»¤empty_ids
        speech_empty_ids = main_model.speech_empty_ids
        is_real_audio_mask = jnp.zeros(audio_tokens_array.shape[1], dtype=bool)

        for ch in range(audio_channels):
            empty_id = speech_empty_ids[ch]
            not_empty = audio_tokens_array[ch, :] != empty_id
            is_real_audio_mask = is_real_audio_mask | not_empty

        num_real_audio = int(jnp.sum(is_real_audio_mask))
        print(f"   çœŸå®éŸ³é¢‘æ—¶é—´æ­¥: {num_real_audio}/{audio_tokens_array.shape[1]}")

        if num_real_audio > 0:
            # è¿‡æ»¤å¹¶è§£ç 
            audio_tokens_array = audio_tokens_array[:, is_real_audio_mask]
            print(f"   è¿‡æ»¤å: {audio_tokens_array.shape}")

            decoded_audio = tokenizer_model.decode(audio_tokens_array)
            print(f"   è§£ç å: {decoded_audio.shape}")

            # ä¿å­˜éŸ³é¢‘
            os.makedirs(output_dir, exist_ok=True)

            try:
                import soundfile as sf
                audio_path = os.path.join(output_dir, "generated_audio.wav")
                audio_np = np.array(decoded_audio[0, 0, :])
                sample_rate = tokenizer_config.sampling_rate
                sf.write(audio_path, audio_np, sample_rate)

                audio_duration = len(audio_np) / sample_rate
                print(f"\nâœ… éŸ³é¢‘å·²ä¿å­˜: {audio_path}")
                print(f"   æ—¶é•¿: {audio_duration:.2f}s")
                print(f"   é‡‡æ ·ç‡: {sample_rate} Hz")
            except Exception as e:
                print(f"\nâŒ ä¿å­˜éŸ³é¢‘å¤±è´¥: {e}")
        else:
            print("\nâš ï¸  è­¦å‘Š: æ²¡æœ‰çœŸå®éŸ³é¢‘å†…å®¹")

    except Exception as e:
        print(f"\nâŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "=" * 70)
    print("âœ… æ¨ç†å®Œæˆ")
    print("=" * 70)


def main():
    """ä¸»å‡½æ•°"""
    # æ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨é»˜è®¤ModelScopeç¼“å­˜ï¼‰
    model_path = os.path.expanduser(
        "~/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-Audio-7B-Instruct"
    )
    tokenizer_path = os.path.expanduser(
        "~/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-Audio-Tokenizer"
    )

    print("=" * 70)
    print("ğŸµ MiMo Audio æœ€å°æ¨ç†è„šæœ¬")
    print("=" * 70)
    print(f"ä¸»æ¨¡å‹: {model_path}")
    print(f"Tokenizer: {tokenizer_path}")
    print()

    # åŠ è½½æ¨¡å‹
    tokenizer_model, tokenizer_config = load_audio_tokenizer(tokenizer_path)
    main_model, config, args, text_tokenizer = load_main_model(model_path)

    # æ‰§è¡Œæ¨ç†
    text_to_speak = "åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚"

    run_inference(
        main_model=main_model,
        tokenizer_model=tokenizer_model,
        text_tokenizer=text_tokenizer,
        config=config,
        args=args,
        tokenizer_config=tokenizer_config,
        text_to_speak=text_to_speak,
        max_steps=100,
    )


if __name__ == "__main__":
    main()
